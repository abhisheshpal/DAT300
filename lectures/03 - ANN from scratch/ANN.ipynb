{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 12 - Implementing a Multi-layer Artificial Neural Network from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"./images/irobot.jpg\" alt=\"iRobot\" style=\"width: 840px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- [Modeling complex functions with artificial neural networks](#Modeling-complex-functions-with-artificial-neural-networks)\n",
    "  - [Single-layer neural network recap](#Single-layer-neural-network-recap)\n",
    "  - [Introducing the multi-layer neural network architecture](#Introducing-the-multi-layer-neural-network-architecture)\n",
    "  - [Activating a neural network via forward propagation](#Activating-a-neural-network-via-forward-propagation)\n",
    "- [Classifying handwritten digits](#Classifying-handwritten-digits)\n",
    "  - [Obtaining the MNIST dataset](#Obtaining-the-MNIST-dataset)\n",
    "  - [Implementing a multi-layer perceptron](#Implementing-a-multi-layer-perceptron)\n",
    "- [Training an artificial neural network](#Training-an-artificial-neural-network)\n",
    "  - [Computing the logistic cost function](#Computing-the-logistic-cost-function)\n",
    "  - [Developing your intuition for backpropagation](#Developing-your-intuition-for-backpropagation)\n",
    "  - [Training neural networks via backpropagation](#Training-neural-networks-via-backpropagation)\n",
    "- [Convergence in neural networks](#Convergence-in-neural-networks)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling complex functions with artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Neural network history begins in the 1940s with Warren McCulloch and Walter Pitt\n",
    "- Rosenblatt's perceptron in the 1950s\n",
    "    - Still in use\n",
    "    - Community lost interest until ...\n",
    "- D.E. Rumelhart G.E. Hinton and R.J. Williams (re)discovered and popularized backpropagation in 1986\n",
    "    - Interest reduced again with lack of sufficient computing power and rise of competing methods\n",
    "- After ups and downs, several important discoveries in the late 200Xs and a revolution from around 2012 (+/-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single-layer neural network recap\n",
    "ADAptive LInear NEuron (Adaline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"./images/12_01.png\" alt=\"Adaline\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Net input:  \n",
    "$z = \\sum_j w_j x_j = \\bf{w^T x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Activation function\n",
    "For Adaline:  \n",
    "$\\phi (z) = z = a$  \n",
    "  \n",
    "(For Logistic Regression:  \n",
    "$\\phi (z) = \\frac{1}{1+e^{-z}} = a$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Threshold (for Adaline):  \n",
    "$\n",
    "\\begin{equation}\n",
    "  \\hat{y}=\\begin{cases}\n",
    "    1  \\text{ if } g(z) \\geq 0\\\\\n",
    "    -1  \\text{ otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weight update\n",
    "Gradient descent:  \n",
    "$\\bf{w:=w} + \\Delta \\bf{w}$, where $\\Delta \\bf{w} = -\\eta \\nabla \\it{J} \\bf{(w)}$,  \n",
    "i.e. the gradient based on the whole training set, taking a step opposite to the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cost function, sum of squared errors (SSE):  \n",
    "$\\it{J} \\bf{(w)} = \\frac{1}{2}\\sum^n_{i=1} (y^{(i)}-\\phi (z^{(i)}))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Partial derivative:  \n",
    "$\\frac{\\partial}{\\partial w_j} \\it{J} (\\bf{w}) = -\\sum_i (y^{(i)} - \\phi (z^{(i)}))x^{(i)}_j$,  \n",
    "where (y, x, a) are (target label, sample and activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from adaline import AdalineGD \n",
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "y_all = pd.DataFrame(iris['target'])\n",
    "X_all = pd.DataFrame(iris['data'])\n",
    "classNames = iris['target_names']\n",
    "\n",
    "# Subset of the Iris data\n",
    "firstClass = 0; secondClass = 1\n",
    "y = y_all[(y_all[0] == firstClass) | (y_all[0] == secondClass)]\n",
    "X = X_all.loc[y.index, [0, 2]].values\n",
    "\n",
    "y = y.values\n",
    "y = np.where(y == 0, -1, 1) # Change from 0,1 to -1,1\n",
    "# Get shape from (100,1) to (100,), needed for boundary plotting function\n",
    "y = y.reshape((np.shape(y)[0],)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:50, 0], X[:50, 1],\n",
    "            color='red', marker='o', label=classNames[firstClass])\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
    "            color='blue', marker='x', label=classNames[secondClass])\n",
    "\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_std = np.copy(X)\n",
    "X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
    "X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "ada = AdalineGD(n_iter=15, eta=0.01)\n",
    "ada.fit(X_std, y)\n",
    "plot_decision_regions(X_std, y, clf=ada, legend=2)\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.title('Adaline on Iris')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimisation\n",
    "- Gradient descent\n",
    "- Stochastic gradient descent (SGD)\n",
    "    - Online learning (random, single sample update)\n",
    "    - Mini-batch learning (random subset update)\n",
    "    - Added noise from random overfit can help avoid local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "import numpy as np\n",
    "\n",
    "def func(x):\n",
    "    return x**2\n",
    "# n_sample=50, eta=0.1 is beautiful\n",
    "# n_sample=20, eta=0.01 struggles\n",
    "def SGD_Sequence_1D(function=func, start=1.0, n_sample=50, eta=0.1, n_updates=20):\n",
    "    sequence = np.zeros([n_updates,1])\n",
    "    values   = np.zeros([n_updates,2])\n",
    "    current  = start\n",
    "    for i in range(n_updates):\n",
    "        # New mini-batch\n",
    "        x = (np.sort(npr.random(n_sample)-0.5)*2) * (abs(start)*2)\n",
    "        \n",
    "        # Function values of mini-batch\n",
    "        y = function(x)\n",
    "        \n",
    "        # Add to sequence\n",
    "        ind = (np.abs(x - current)).argmin()\n",
    "        sequence[i] = current\n",
    "        values[i,:] = [x[ind], y[ind]]\n",
    "\n",
    "        # Find descrete derivative closest to current\n",
    "        gr = np.gradient(y, x)\n",
    "\n",
    "        # Update current\n",
    "        current = current - eta*gr[ind]\n",
    "    \n",
    "    return(sequence, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def SGD_plot(n_updates=10):\n",
    "    npr.seed(1)\n",
    "    seq, val = SGD_Sequence_1D(n_updates=n_updates)\n",
    "\n",
    "    points = val.reshape(-1,1,2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    cols = np.arange(n_updates*1.0)\n",
    "\n",
    "    fig, axs = plt.subplots()\n",
    "    norm = plt.Normalize(cols.min(), cols.max())\n",
    "    lc = LineCollection(segments, cmap='viridis', norm=norm)\n",
    "    lc.set_array(cols)\n",
    "    lc.set_linewidth(2)\n",
    "    line = axs.add_collection(lc)\n",
    "    fig.colorbar(line, ax=axs)\n",
    "    axs.set_xlim(-1.2, 1.2)\n",
    "    axs.set_ylim(0, 1.2)\n",
    "    plt.show()\n",
    "\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "interact(SGD_plot, n_updates=widgets.IntSlider(min=2, max=20, step=1, value=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducing the multi-layer neural network architecture\n",
    "First out: the Multilayer Perceptron (MLP)\n",
    "- Fully connected network\n",
    "- $a_h^{(h)}$ should be $a_d^{(h)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"./images/12_02.png\" alt=\"Multilayer Percetron\" style=\"width: 600px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/12_02.png\" alt=\"Multilayer Percetron\" style=\"width: 600px;\"/>  \n",
    "- $a_i^{(l)}$ = activation unit $i$ in $l$-th layer. \n",
    "- $w_{i,j}^{(l)}$ = weight connecting unit $i$ in $l-1$-th layer with unit $j$ in $l$-th layer. These will be grouped into a matrix $W^{(l)}$ later. \n",
    "- $i=0$ are bias units (constant/intercept).  \n",
    "More than one hidden layer => **deep artificial neural network**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First layer activation\n",
    "$a^{(in)} = \\begin{bmatrix}a^{(in)}_0 \\\\ a^{(in)}_1 \\\\ \\vdots \\\\ a^{(in)}_m\\end{bmatrix} = \\begin{bmatrix}1 \\\\ x^{(in)}_1 \\\\ \\vdots \\\\ x^{(in)}_m\\end{bmatrix}$,  \n",
    "  \n",
    "i.e. the input and an intercept.  \n",
    "The bias units of all layers are grouped together as a separate vector later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adding layers\n",
    "- More layers give higher flexibility\n",
    "    - Weight matrices, $W^{(l)}$, will have dimension $m \\times d$, where $m$ is the number of units in the previous layer (including the bias) and $d$ is the number of units in the $l$-th layer.\n",
    "- Increases possibility of over-fitting\n",
    "- Increases number of parameters to estimate (good and bad)\n",
    "- Error gradients increasingly small\n",
    "    - Deep learning tools/tricks to overcome this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiclass classification\n",
    "- Multiple output units\n",
    "- Corresponds to one-hot encoding\n",
    "- Typically a soft-max function on the output:\n",
    "  - Class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3-4-3 multilayer perceptron\n",
    "<img src=\"./images/12_03.png\" alt=\"3-4-3 Perceptron\" style=\"width: 500px;\"/>\n",
    "- \"Describe me, please!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activating a neural network via forward propagation\n",
    "  \n",
    "Three main steps of fitting an MLP:\n",
    "1. Forward propagate patterns of the training data from input to output.\n",
    "2. Calculate error based on a cost function.\n",
    "3. Backpropagate the error through derivatives with respect to each weight and update the model.  \n",
    "\n",
    "Repeat for several epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Forward propagation\n",
    "Calculation of the first actiavation unit in the (first) hidden layer:  \n",
    "  \n",
    "$$z_1^{(h)} = a_0^{(in)} w_{0,1}^{(h)} + a_1^{(in)} w_{1,1}^{(h)} + \\dots + a_m^{(in)} w_{m,1}^{(h)}$$  \n",
    "$$a_1^{(h)} = \\phi(z_1^{(h)})$$  \n",
    "  \n",
    "$\\phi(\\cdot)$ is the activation function, differentiable, often non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation function examples\n",
    "The basic sigmoid transforms unbounded inputs $z$ to the range $(0,1)$:\n",
    "<img src=\"./images/12_04.png\" alt=\"Sigmoid activation\" style=\"width: 500px;\"/>  \n",
    "  \n",
    "Rectified Linear Units - ReLU (and its siblings) have taken over as the default activiation function in many applications due to their simplicity and beneficial properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compact notation\n",
    "For easier readability and more efficient calculations we change from sum-notation to matrix notation. Collect all activation units of layer $l-1$ and all weights of for the transition to layer $l$ into matrices:  \n",
    "$$\\bf{Z}^\\it{(l)} = \\bf{A}^\\it{(l-1)} \\bf{W}^\\it{(l)}$$\n",
    "  \n",
    "$$\\bf{A}^\\it{(l)} = \\phi \\left(\\bf{Z}^\\it{(l)} \\right)$$\n",
    "\n",
    "(class eagerly raises hands to define the dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classifying handwritten digits\n",
    "Revisit MNIST (Modified National Institute of Standards and Technology) database from compulsory assignment 1 in DAT200.  \n",
    "https://en.wikipedia.org/wiki/MNIST_database  \n",
    "  \n",
    "<img src=\"./images/MnistExamples.png\" alt=\"MNIST digits\" style=\"width: 500px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MNIST\n",
    "- Derived from the NIST database of B/W handwritten images\n",
    "- 28x28 bounding box, anti-aliased grayscale images\n",
    "- 60,000 training images and 10,000 test images \n",
    "- Convolutional neural networks have achieved 0.21% error rate with expansion of the training data (1.6% record on raw data).\n",
    "- Extended MNIST (EMNIST) published in 2017 containing 240,000 training images and 40,000 testing images of digits and characters (https://www.nist.gov/itl/iad/image-group/emnist-dataset).\n",
    "- Further MNIST-like databases like Zalando Research's fashion images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obtaining the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n",
    "\n",
    "- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 samples)\n",
    "- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n",
    "- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 samples)\n",
    "- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n",
    "\n",
    "In this section, we will only be working with a subset of MNIST, thus, we only need to download the training set images and training set labels. \n",
    "\n",
    "After downloading the files, simply run the next code cell to unzip the files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This code cell unzips mnist\n",
    "import sys\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "if (sys.version_info > (3, 0)):\n",
    "    writemode = 'wb' # add binary to the mode\n",
    "else:\n",
    "    writemode = 'w'\n",
    "\n",
    "# ------ Put the gz-files in the data directory before running this ------ \n",
    "zipped_mnist = [f for f in os.listdir('data/') if f.endswith('ubyte.gz')]\n",
    "print(zipped_mnist)\n",
    "for z in zipped_mnist:\n",
    "    with gzip.GzipFile('data/'+z, mode='rb') as decompressed, open('data/'+z[:-3], writemode) as outfile:\n",
    "        outfile.write(decompressed.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath: # Read image labels\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath: # Read images, reshape to matrix format and \"floatify\"\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2 # Normalize to [-1,1]\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('data', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = load_mnist('data', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize the first digit of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([]); ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize 25 different versions of \"7\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = X_train[y_train == 7][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([]); ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Save data for later\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez.html  \n",
    "_savez_ uses a compressed (zip) NumPy array format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savez_compressed('data/mnist_scaled.npz', \n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mnist = np.load('data/mnist_scaled.npz')\n",
    "mnist.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Split and delete list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = [mnist[f] for f in ['X_train', 'y_train', \n",
    "                                    'X_test', 'y_test']]\n",
    "\n",
    "del mnist\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing a multi-layer perceptron\n",
    "- The following code is too large for slide presentation\n",
    "- It contains parts that have not yet been introduced, e.g. backpropagation\n",
    "    - We will revisit the code later\n",
    "- Training the MLP on the MNIST data requires around 1.5 seconds per epoch on Liland's computer. Start calculations before going through the code if recommended 200 epochs are used (~5 minutes computing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "    l2 : float (default: 0.)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0. (default)\n",
    "    epochs : int (default: 100)\n",
    "        Number of passes over the training set.\n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "    minibatch_size : int (default: 1)\n",
    "        Number of training samples per minibatch used in the stochastic gradient descent.\n",
    "        (One gradient update per minibatch)\n",
    "    seed : int (default: None)\n",
    "        Random seed for initalizing weights and shuffling.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    eval_ : dict\n",
    "      Dictionary collecting the cost, training accuracy,\n",
    "      and validation accuracy for each epoch during training.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 l2=0., epochs=100, eta=0.001,\n",
    "                 shuffle=True, minibatch_size=1, seed=None):\n",
    "\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "    def _onehot(self, y, n_classes):\n",
    "        \"\"\"Encode labels into one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : array, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot : array, shape = (n_samples, n_labels)\n",
    "\n",
    "        \"\"\"\n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.\n",
    "        return onehot.T\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\"\"\"\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"Compute forward propagation step\"\"\"\n",
    "\n",
    "        # step 1: net input of hidden layer\n",
    "        # [n_samples, n_features] dot [n_features, n_hidden]\n",
    "        # -> [n_samples, n_hidden]\n",
    "        z_h = np.dot(X, self.w_h) + self.b_h\n",
    "\n",
    "        # step 2: activation of hidden layer\n",
    "        a_h = self._sigmoid(z_h)\n",
    "\n",
    "        # step 3: net input of output layer\n",
    "        # [n_samples, n_hidden] dot [n_hidden, n_classlabels]\n",
    "        # -> [n_samples, n_classlabels]\n",
    "\n",
    "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
    "\n",
    "        # step 4: activation output layer\n",
    "        a_out = self._sigmoid(z_out)\n",
    "\n",
    "        return z_h, a_h, z_out, a_out\n",
    "\n",
    "    def _compute_cost(self, y_enc, output):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_samples, n_labels)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_samples, n_output_units]\n",
    "            Activation of the output layer (forward propagation)\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost\n",
    "\n",
    "        \"\"\"\n",
    "        L2_term = (self.l2 *\n",
    "                   (np.sum(self.w_h ** 2.) +\n",
    "                    np.sum(self.w_out ** 2.)))\n",
    "\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1. - y_enc) * np.log(1. - output)\n",
    "        cost = np.sum(term1 - term2) + L2_term\n",
    "        \n",
    "        # If you are applying this cost function to other\n",
    "        # datasets where activation\n",
    "        # values maybe become more extreme (closer to zero or 1)\n",
    "        # you may encounter \"ZeroDivisionError\"s due to numerical\n",
    "        # instabilities in Python & NumPy for the current implementation.\n",
    "        # I.e., the code tries to evaluate log(0), which is undefined.\n",
    "        # To address this issue, you could add a small constant to the\n",
    "        # activation values that are passed to the log function.\n",
    "        #\n",
    "        # For example:\n",
    "        #\n",
    "        # term1 = -y_enc * (np.log(output + 1e-5))\n",
    "        # term2 = (1. - y_enc) * np.log(1. - output + 1e-5)\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        z_h, a_h, z_out, a_out = self._forward(X)\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X_train : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y_train : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        X_valid : array, shape = [n_samples, n_features]\n",
    "            Sample features for validation during training\n",
    "        y_valid : array, shape = [n_samples]\n",
    "            Sample labels for validation during training\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        n_output = np.unique(y_train).shape[0]  # number of class labels\n",
    "        n_features = X_train.shape[1]\n",
    "\n",
    "        ########################\n",
    "        # Weight initialization\n",
    "        ########################\n",
    "\n",
    "        # weights for input -> hidden\n",
    "        self.b_h = np.zeros(self.n_hidden)\n",
    "        self.w_h = self.random.normal(loc=0.0, scale=0.1,\n",
    "                                      size=(n_features, self.n_hidden))\n",
    "\n",
    "        # weights for hidden -> output\n",
    "        self.b_out = np.zeros(n_output)\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1,\n",
    "                                        size=(self.n_hidden, n_output))\n",
    "\n",
    "        epoch_strlen = len(str(self.epochs))  # for progress formatting\n",
    "        self.eval_ = {'cost': [], 'train_acc': [], 'valid_acc': []}\n",
    "\n",
    "        y_train_enc = self._onehot(y_train, n_output)\n",
    "\n",
    "        # iterate over training epochs\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # iterate over minibatches\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "\n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size +\n",
    "                                   1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "\n",
    "                # forward propagation\n",
    "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
    "\n",
    "                ##################\n",
    "                # Backpropagation\n",
    "                ##################\n",
    "\n",
    "                # [n_samples, n_classlabels]\n",
    "                sigma_out = a_out - y_train_enc[batch_idx]\n",
    "\n",
    "                # [n_samples, n_hidden]\n",
    "                sigmoid_derivative_h = a_h * (1. - a_h)\n",
    "\n",
    "                # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
    "                # -> [n_samples, n_hidden]\n",
    "                sigma_h = (np.dot(sigma_out, self.w_out.T) *\n",
    "                           sigmoid_derivative_h)\n",
    "\n",
    "                # [n_features, n_samples] dot [n_samples, n_hidden]\n",
    "                # -> [n_features, n_hidden]\n",
    "                grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
    "                grad_b_h = np.sum(sigma_h, axis=0)\n",
    "\n",
    "                # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
    "                # -> [n_hidden, n_classlabels]\n",
    "                grad_w_out = np.dot(a_h.T, sigma_out)\n",
    "                grad_b_out = np.sum(sigma_out, axis=0)\n",
    "\n",
    "                # Regularization and weight updates\n",
    "                delta_w_h = (grad_w_h + self.l2*self.w_h)\n",
    "                delta_b_h = grad_b_h # bias is not regularized\n",
    "                self.w_h -= self.eta * delta_w_h\n",
    "                self.b_h -= self.eta * delta_b_h\n",
    "\n",
    "                delta_w_out = (grad_w_out + self.l2*self.w_out)\n",
    "                delta_b_out = grad_b_out  # bias is not regularized\n",
    "                self.w_out -= self.eta * delta_w_out\n",
    "                self.b_out -= self.eta * delta_b_out\n",
    "\n",
    "            #############\n",
    "            # Evaluation\n",
    "            #############\n",
    "\n",
    "            # Evaluation after each epoch during training\n",
    "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
    "            \n",
    "            cost = self._compute_cost(y_enc=y_train_enc,\n",
    "                                      output=a_out)\n",
    "\n",
    "            y_train_pred = self.predict(X_train)\n",
    "            y_valid_pred = self.predict(X_valid)\n",
    "\n",
    "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /\n",
    "                         X_train.shape[0])\n",
    "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) /\n",
    "                         X_valid.shape[0])\n",
    "\n",
    "            sys.stderr.write('\\r%0*d/%d | Cost: %.2f '\n",
    "                             '| Train/Valid Acc.: %.2f%%/%.2f%% ' %\n",
    "                             (epoch_strlen, i+1, self.epochs, cost,\n",
    "                              train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "\n",
    "            self.eval_['cost'].append(cost)\n",
    "            self.eval_['train_acc'].append(train_acc)\n",
    "            self.eval_['valid_acc'].append(valid_acc)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train on MNIST\n",
    "- Training is expensive, especially for deep neural networks\n",
    "    - Checking performance and tendencies of overfitting on a limited number of epochs and adjusting parameters can save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Around 1.5 seconds per epoch on Liland's computer\n",
    "n_epochs = 200\n",
    "nn = NeuralNetMLP(n_hidden=100, \n",
    "                  l2=0.01, \n",
    "                  epochs=n_epochs, \n",
    "                  eta=0.0005,\n",
    "                  minibatch_size=100, \n",
    "                  shuffle=True,\n",
    "                  seed=1)\n",
    "\n",
    "nn.fit(X_train=X_train[:55000], y_train=y_train[:55000],\n",
    "       X_valid=X_train[55000:], y_valid=y_train[55000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize cost over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(nn.epochs), nn.eval_['cost'])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Still decreasing after 200 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(nn.epochs), nn.eval_['train_acc'], label='training')\n",
    "plt.plot(range(nn.epochs), nn.eval_['valid_acc'], label='validation', linestyle='--')\n",
    "plt.ylabel('Accuracy'); plt.xlabel('Epochs')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Overfitting from ~50 epochs. Refit with higher regularization, e.g. L2 = 0.1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = nn.predict(X_test)\n",
    "acc = (np.sum(y_test == y_test_pred)\n",
    "       .astype(np.float) / X_test.shape[0])\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Not bad for the autumn's first neural network. Still a little gap to the 98.7% reported in Wikipedia and 99.71% with pre-processing/data augmentation.  \n",
    "- Tuning tricks include number of hidden units, degree of regularization, learning rate, adaptive learning rate, momentum learning and dropout. More on these later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error analysis\n",
    "- Mentioned at the end of Feature Selection\n",
    "- Assess some of the large errors or wrong classifications\n",
    "- Get an idea of what can be improved\n",
    "    - Tuning the algorithm\n",
    "    - Pre-processing (image improvements/morphing)?\n",
    "    - Extend training data with samples from EMNIST?\n",
    "- Frown at impossible tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "miscl_img = X_test[y_test != y_test_pred][:25]\n",
    "correct_lab = y_test[y_test != y_test_pred][:25]\n",
    "miscl_lab = y_test_pred[y_test != y_test_pred][:25]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = miscl_img[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title('%d) t: %d p: %d' % (i+1, correct_lab[i], miscl_lab[i]))\n",
    "\n",
    "ax[0].set_xticks([]); ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training an artificial neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We will go deeper into:  \n",
    "- The cost function\n",
    "- Backpropagation\n",
    "- Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the logistic cost function\n",
    "The implementation the used same logistic cost functions as Chapter 3 as a basis:\n",
    "$$\\it{J} (w) = -\\sum_{i=1}^{n} y^{[i]} log(a^{[i]}) + \\left( 1-y^{[i]}\\right) log(1-a^{[i]})$$\n",
    "  \n",
    "... where $a^{[i]}$ is output of a node for sample [i] in the output layer:\n",
    "$$a^{[i]} = \\phi(z^{[i]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### L2 penalization\n",
    "$$L2 = \\lambda ||w||^2_2 = \\lambda \\sum_{j=1}^m w^2_j,$$\n",
    "  \n",
    "regularizes all weights except the bias units for the node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Adding this to the cost results in:\n",
    "  \n",
    "$$\\it{J} (w) = -\\left[ \\sum_{i=1}^{n} y^{[i]} log(a^{[i]}) + \\left( 1-y^{[i]}\\right) log(1-a^{[i]}) \\right] + \\frac{\\lambda}{2} ||w||^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cost function for all samples and nodes\n",
    "$$\\it{J} (w) = -\\sum_{i=1}^{n}\\sum_{j=1}^{t} y_j^{[i]} log(a_j^{[i]}) + \\left( 1-y_j^{[i]}\\right) log(1-a_j^{[i]})$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Adding the L2 norm penalization finally yields:  \n",
    "  \n",
    "$$\\it{J} (w) = -\\left[ \\sum_{i=1}^{n}\\sum_{j=1}^{t} y_j^{[i]} log(a_j^{[i]}) + \\left( 1-y_j^{[i]}\\right) log(1-a_j^{[i]}) \\right] + \\frac{\\lambda}{2} \\sum_{l=1}^{L-1}\\sum_{i=1}^{u_l}\\sum_{j=1}^{u_{l+1}}\\left(w_{j,i}^{(l)}\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def _compute_cost(self, y_enc, output):\n",
    "    L2_term = (self.l2 *\n",
    "               (np.sum(self.w_h ** 2.) +\n",
    "                np.sum(self.w_out ** 2.)))\n",
    "\n",
    "    term1 = -y_enc * (np.log(output))\n",
    "    term2 = (1. - y_enc) * np.log(1. - output)\n",
    "    cost = np.sum(term1 - term2) + L2_term\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-hot encoding representation\n",
    "The activation of the third layer and the target class for __a particular__ sample may look like this:\n",
    "$$a^{(out)} = \\begin{bmatrix}0.1 \\\\ 0.9 \\\\ \\vdots \\\\ 0.3\\end{bmatrix}, y = \\begin{bmatrix}0 \\\\ 1 \\\\ \\vdots \\\\ 0\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    z_h, a_h, z_out, a_out = self._forward(X)\n",
    "    y_pred = np.argmax(z_out, axis=1) # argmax on z_out and a_out is equivalent\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cost minimization\n",
    "... will be performed using the partial derivative of the parameters of $W$ with respect to each weight for every layer:\n",
    "$$\\frac{\\partial}{\\partial w_{j,i}^{(l)}}J(W)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/12_10.png\" alt=\"W in layers\" style=\"width: 400px;\"/>\n",
    "... where each $W^{(l)}$ may have different dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Developing your intuition for backpropagation\n",
    "- Backpropagation is still one of the most popular algorithms to train ANNs\n",
    "- Challenging with high dimensions and non-convex cost function (local minima)\n",
    "- Main building block: \"The chain rule\"\n",
    "    - Derivative of a nested function\n",
    "$$\\frac{d}{dx}[f(g(x))] = \\frac{df}{dg}\\cdot\\frac{dg}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$F(x) = f(g(h(u(v(x)))))$$\n",
    "  \n",
    "$$\\frac{dF}{dx} = \\frac{d}{dx}F(x) = \\frac{d}{dx}[f(g(h(u(v(x)))))] = \\frac{df}{dg}\\cdot\\frac{dg}{dh}\\cdot\\frac{dh}{du}\\cdot\\frac{du}{dv}\\cdot\\frac{dv}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deriving everything by hand\n",
    "- ... is something we can avoid\n",
    "- Automatic differentiation\n",
    "    - Simple function derivatives can be coded as matrices\n",
    "    - Starting from the left would yields a number of matrix products times a vector (forward mode)\n",
    "    - Starting from the right all products are of the form matrix times vector (reverse mode)\n",
    "        - Dimension collapse => high speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training neural networks via backpropagation\n",
    "Forward-propagation of input features through:  \n",
    "$Z^{(h)}=A^{(in)}W^{(h)}$ : net input of the hidden layer  \n",
    "$A^{(h)}=\\phi(Z^{(h)})$ : activation of the hidden layer  \n",
    "$Z^{(out)}=A^{(h)}W^{(out)}$ : net input of the output layer  \n",
    "$A^{(out)}=\\phi(Z^{(out)})$ : activation of the output layer  \n",
    "<img src=\"./images/12_11.png\" alt=\"Feed forward\" style=\"width: 400px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error propagation\n",
    "Backpropagation - propagate the error from right to left.  \n",
    "Calculate the error vector (nxt): \n",
    "$$\\delta^{(out)} = a^{(out)}-y.$$  \n",
    "  \n",
    "Calculate the error of the hidden layer by propagation, transforming the errors backwards through the network:\n",
    "$$\\delta^{(h)} = \\delta^{(out)}(W^{(out)})^T \\odot \\frac{\\partial\\phi(z^{(h)})}{\\partial z^{(h)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The derivative of the sigmoid function:  \n",
    "$$\\frac{\\partial\\phi(z^{(h)})}{\\partial z^{(h)}} = (a^{(h)} \\odot (1-a^{(h)}))$$\n",
    "  \n",
    "... because:  \n",
    "$$\\phi'(z) = \\frac{\\partial}{\\partial z} \\left( \\frac{1}{1+e^{-z}} \\right) = \\frac{1}{1+e^{-z}} - \\left( \\frac{1}{1+e^{-z}} \\right)^2$$  \n",
    "  \n",
    "$$= \\phi(z) - (\\phi(z))^2 = \\phi(z)(1-\\phi(z)) = a(1-a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error propagation, hidden layer\n",
    "$$\\delta^{(h)} = \\delta^{(out)}(W^{(out)})^T \\odot \\frac{\\partial\\phi(z^{(h)})}{\\partial z^{(h)}}$$  \n",
    "  \n",
    "$$ = \\delta^{(out)}(W^{(out)})^T \\odot (a^{(h)} \\odot (1-a^{(h)}))$$  \n",
    "  \n",
    "... where $\\delta^{(out)}$ has dimensions (nxt), $W^{(out)}$ has dimensions (hxt) and $\\delta^{(h)}$ has dimensions (nxh).  \n",
    "(sigma_h in the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def errors():\n",
    "    sigma_out = a_out - y_train_enc[batch_idx]\n",
    "\n",
    "    # [n_samples, n_hidden]\n",
    "    sigmoid_derivative_h = a_h * (1. - a_h)\n",
    "\n",
    "    # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
    "    # -> [n_samples, n_hidden]\n",
    "    sigma_h = (np.dot(sigma_out, self.w_out.T) *\n",
    "               sigmoid_derivative_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Derivative of the cost function\n",
    "$$\\frac{\\partial}{\\partial w_{i,j}^{(out)}} J(W) = a_j^{(h)}\\delta_i^{(out)}$$\n",
    "$$\\frac{\\partial}{\\partial w_{i,j}^{(h)}} J(W) = a_j^{(in)}\\delta_i^{(h)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... in vectorized format with regularization terms:  \n",
    "$$\\Delta^{(h)} = \\left( A^{(in)} \\right)^T \\delta^{(h)} + \\lambda^{(h)}$$\n",
    "$$\\Delta^{(out)} = \\left( A^{(h)} \\right)^T \\delta^{(out)} + \\lambda^{(out)}$$\n",
    "(delta_w_out and delta_w_h in the code, grad_w_.. + regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gradients():\n",
    "    ## GRADIENTS ##\n",
    "    # [n_features, n_samples] dot [n_samples, n_hidden]\n",
    "    # -> [n_features, n_hidden]\n",
    "    grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
    "    grad_b_h = np.sum(sigma_h, axis=0)\n",
    "\n",
    "    # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
    "    # -> [n_hidden, n_classlabels]\n",
    "    grad_w_out = np.dot(a_h.T, sigma_out)\n",
    "    grad_b_out = np.sum(sigma_out, axis=0)\n",
    "\n",
    "    # Regularization and weight updates\n",
    "    delta_w_h = (grad_w_h + self.l2*self.w_h)\n",
    "    delta_b_h = grad_b_h # bias is not regularized\n",
    "\n",
    "    delta_w_out = (grad_w_out + self.l2*self.w_out)\n",
    "    delta_b_out = grad_b_out  # bias is not regularized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weight update\n",
    "$$W^{(l)}:=W^{(l)} - \\eta \\Delta^{(l)}$$  \n",
    "  \n",
    "A small step in the opposite direction of the gradient (for each layer in the network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def backprop():\n",
    "    ## ERRORS ##\n",
    "    # [n_samples, n_classlabels]\n",
    "    sigma_out = a_out - y_train_enc[batch_idx]\n",
    "\n",
    "    # [n_samples, n_hidden]\n",
    "    sigmoid_derivative_h = a_h * (1. - a_h)\n",
    "\n",
    "    # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
    "    # -> [n_samples, n_hidden]\n",
    "    sigma_h = (np.dot(sigma_out, self.w_out.T) *\n",
    "               sigmoid_derivative_h)\n",
    "\n",
    "    ## GRADIENTS ##\n",
    "    # [n_features, n_samples] dot [n_samples, n_hidden]\n",
    "    # -> [n_features, n_hidden]\n",
    "    grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
    "    grad_b_h = np.sum(sigma_h, axis=0)\n",
    "\n",
    "    # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
    "    # -> [n_hidden, n_classlabels]\n",
    "    grad_w_out = np.dot(a_h.T, sigma_out)\n",
    "    grad_b_out = np.sum(sigma_out, axis=0)\n",
    "\n",
    "    # Regularization and weight updates\n",
    "    delta_w_h = (grad_w_h + self.l2*self.w_h)\n",
    "    delta_b_h = grad_b_h # bias is not regularized\n",
    "    self.w_h -= self.eta * delta_w_h\n",
    "    self.b_h -= self.eta * delta_b_h\n",
    "\n",
    "    delta_w_out = (grad_w_out + self.l2*self.w_out)\n",
    "    delta_b_out = grad_b_out  # bias is not regularized\n",
    "    self.w_out -= self.eta * delta_w_out\n",
    "    self.b_out -= self.eta * delta_b_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/12_12.png\" alt=\"Back propagation\" style=\"width: 500px;\"/>  \n",
    "Go through fit function one more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convergence in neural networks\n",
    "- Gradient descent with mini-batch instead of stochastic gradient descent (Ch. 2, p.44)\n",
    "    - Efficient vectorized code\n",
    "    - Like a \"meningsmling ved valg\" / election poll\n",
    "    - Still stochastic, hopefully jumping out of local minima\n",
    "- Learning rate: fixed rate $\\eta$\n",
    "    - Deep Learning brings adaptive learning rate, learning rate with decay, learning rate with restarts, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/12_13.png\" alt=\"Convergence\" style=\"width: 500px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "- Expanded perceptron to multilayer perceptron\n",
    "    - Varying numbers of units (input, hidden, hidden, ..., output)\n",
    "    - SGD -> mini-batch\n",
    "    - Forward propagation of features\n",
    "    - Backpropagation of errors (chain rule)\n",
    "    - Developed the gradient of the cost function for all weights in the network\n",
    "- Non-exhaustive list of activation functions on page 450."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
